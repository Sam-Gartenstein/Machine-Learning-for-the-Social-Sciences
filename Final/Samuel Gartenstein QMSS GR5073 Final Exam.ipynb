{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Samuel Gartenstein QMSS GR5073 Final Exam"
      ],
      "metadata": {
        "id": "zXxAT8ja3po8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "\n",
        "From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n",
        "\n"
      ],
      "metadata": {
        "id": "CZF4sGm2inNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "The models we learned this semester that are useful for ruling out alternative explanations through control variables and allow us to observe substantively meaningful information from our coefficients were linear, logistic, lasso, and ridge regression.\n",
        "\n",
        "From a traditional social science approach, adding controls to our model serves the purpose of improving our ability to explain our target feature. This may reduce the explanatory power of our primary predictor. This reduction would happen if our controls were correlated with our primary predictor or better explains our target feature. While social scientists and statisticians rely on p-values (in a frequentist framework), machine leaners normally analyze the magnitude of their coefficients. For example, if there is multicollinearity or omitted variable bias, the coefficient of the model may be misrepresented. As a result, adding controls may allow machine learners to reduce the magnitude the coefficient(s) that do not significantly contribute to the model. This in turn allows them to rule out alternative explanations as well as analyze the predictors that have an impact on our target feature.\n",
        "\n",
        "It is worth noting that adding control variables may not always be effective. In fact, they can diminish our ability generalize to new data. A model that has too many controls may overfit to training data. In this context, machine learning practitioners utilize lasso or ridge regression. Lasso and ridge regression are regularization techniques that shrink coefficients of predictors to near or zero (zero in the case of lasso) if they have multicollinearity or minimal explanatory power. This allows machine learning practitioners to minimize the risk of overfitting, thereby increasing their ability to parse out their model(s) most important features.\n",
        "\n"
      ],
      "metadata": {
        "id": "4oqHv-afiqxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "\n",
        "Describe the main differences between supervised and unsupervised learning."
      ],
      "metadata": {
        "id": "NZ-nuaQdjpuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "In supervised machine learning, machine learning practitioners have pre-defined input and output variables. Although the choices of these features depend on the context, there is always a distinction between independent variable(s) and a dependent variable. Supervised machine learning models include, but not limited to linear and logistic regression, SVM, and tree models.\n",
        "\n",
        "On the other hand, in unsupervised machine learning, there is not a pre-defined set of independent or dependent variables. Rather, there are only features that are normally unlabeled. As a result, machine learning practitioners use algorithms such as PCA (Principal Component Analysis) and clustering to derive meaning from the features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9P60QNjUj5nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "\n",
        "Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
        "\n"
      ],
      "metadata": {
        "id": "yz8AqapylRVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "I believe that machine learning practitioners primarily use unsupervised machine learning since data tends to be unlabeled and unstructured. During a lecture, Nina Lerner shared several real-world problems she solves on her job, one of which is extracting information from consumer receipts. In such a case, there are no pre-defined input or outputs; rather data scientists must utilize unsupervised machine learning to derive meaningful information from their consumers.\n",
        "\n",
        "I also believe that another primary goal of unsupervised machine learning is to make data suitable for supervised machine learning. I would not call supervised machine learning \"secondary.\" In fact, I believe that machine learning practitioners would rather have an ordered dataset where they can apply regression, SVM, or ensemble make predictions about a target feature. However, machine learning practitioners are bound by the structure of their data. Unsupervised machine learning, such as PCA, serve to reduce the dimensionality and extract important features for subsequent supervised machine learning models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VKADg4kdlxje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "\n",
        "Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?\n",
        "\n"
      ],
      "metadata": {
        "id": "h5S0p4Zjmxuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "The unsupervised learning models we covered this semester were PCA and Clustering (Kmeans and Hierarchal).\n",
        "\n",
        "In PCA, the goal is to use reduce the dimensions of our data. This is done by projecting our data onto axes, principal components, based off variance. As a result, we can identify components based off variance, determining influential features in our data.\n",
        "\n",
        "In clustering, our goal is to group similar observations of data. For Kmeans clustering achieves this by pre-defining a number of clusters and then minimize the variation within those clusters. Hierarchal clustering utilizes a dendrogram, a tree-like structure, to partition the data based off similarity, and then choose the optimal number of clusters. Both aim to cluster data by grouping those with similar patterns.\n",
        "\n",
        "Overall, the primary difference is that PCA seeks to reduce the dimensionality of the dataset and clustering seeks to group similar sets of observations. However, both serve to derive meaning from unlabeled and/or unstructured data.\n"
      ],
      "metadata": {
        "id": "-a_FAym2m8_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "\n",
        " What are the main benefits of using Principal Components Analysis?"
      ],
      "metadata": {
        "id": "5hkfJsJ9oE7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "The primary benefit of using Principal Components Analysis that it reduces high dimensional data (data with a high number of features). This reduction is achieved by projecting our data onto axes, principal components, based off variance. After, we can retain features that have a high variance. The technique allows for dimensionality reduction while keeping features that relatively high degree of explanatory power.\n",
        "\n",
        "Several advantages follow from reducing the data while keeping the most important features. First, since PCA eliminates features that provide little information, it decreases the risk of overfitting, thereby increasing predictive performance for subsequent ML models. Second, by reducing features with high multicollinearity, PCA helps increase the predictive performance of subsequent ML models. Third, since the unnecessary and/or highly correlated variables are eliminated, utilizing PCA will speed up computation for subsequent ML models.\n"
      ],
      "metadata": {
        "id": "_q1O4e3zoJTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "\n",
        "Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n",
        "\n"
      ],
      "metadata": {
        "id": "6YWiz592sqrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "The first difference between the deep multilayer perceptron network (MPN) and convolutional neural network (CNN) model is the number of connected layers. The MPN has an input layer, one or more hidden layers, and output layer. CNN have more layers, consisting of the convolution layer where the features are filtered, a pooling layer for shrinking data, and fully connected layer.\n",
        "\n",
        "The second difference is that in CNN can filter the data. That is, we can create a new matrix with certain weights, which is called the convolution kernel. We partition our primary data matrix into overlapping subsets and multiply them by our filter. We take the sum of the element-wise multiplication and put it into a new matrix. This allows us to extract patterns and information about our features. Deep MPN does not have this feature, meaning that we cannot filter the data.  \n",
        "\n",
        "The third difference is that there CNN allows dimension reduction. This is done in the pooling layer, which occurs after we filter our data in the convolutional layer. In pooling, the filtered matrix is divided into non-overlapping subsets. After, we reduce our dimensions based off values in the submatrices. This can be done by taking the maximum element in each submatrix, or the average of all the elements, and storing the output into a new smaller matrix. Like filtering, deep MPN does not have this functionality, meaning we cannot reduce the dimensionality of our data.\n"
      ],
      "metadata": {
        "id": "0etH_zZJujbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3yfx3XWq-fib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For Questions 7 and 8, set input shape as (784,), meaning there are 784 features\n",
        "For Question 7 through 10, I set the learning rate to 0.0001, the loss to categorical crossentropy,\n",
        "and used accuracy as the scoring metric.\n",
        "'''\n",
        "\n",
        "model = Sequential([\n",
        "    #First hidden layer with 50 neurons\n",
        "    Dense(50, input_shape=input_shape=(784,)),\n",
        "    Activation('relu'),\n",
        "\n",
        "    #Second hidden layer with 100 neurons\n",
        "    Dense(100),\n",
        "    Activation('relu'),\n",
        "\n",
        "    #Third hidden layer with 150 neurons\n",
        "    Dense(150),\n",
        "    Activation('relu'),\n",
        "\n",
        "    #Output layer set to classify 5 categorical variables\n",
        "    Dense(5),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "#Learning rate for optimization, using stochastic gradient descent\n",
        "sgd = SGD(lr=0.0001)\n",
        "\n",
        "#Compiling model with\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "1OjhJa08ioIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n",
        "\n"
      ],
      "metadata": {
        "id": "_m8b9HfKEd9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    #First hidden layer with 75 neurons\n",
        "    Dense(75, input_shape=('''Number of Features''',)),\n",
        "    Activation('relu'),\n",
        "\n",
        "    #Second hidden layer with 150 neurons\n",
        "    Dense(150),\n",
        "    Activation('relu'),\n",
        "\n",
        "    #Output layer set classify binary dependent variable\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'), #Activated with sigmoid function\n",
        "])\n",
        "\n",
        "#Learning rate for optimization, using stochastic gradient descent\n",
        "sgd = SGD(lr=0.0001)\n",
        "\n",
        "#Compiling model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "i5Sb6e0kFl1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Question 9\n",
        "\n",
        " Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n",
        "\n"
      ],
      "metadata": {
        "id": "EJUqg3xqHp7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For Questions 9 and 10, I made the kernal/filter size 3 by 3, and the input shape 28 by 28 by 1 for all convolutional layers\n",
        "'''\n",
        "\n",
        "#First  convolutional layer with 16 filters\n",
        "model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #max pooling with a 2 by 2 filter\n",
        "\n",
        "#Second  convolutional layer with 28 filters\n",
        "model.add(Conv2D(28, (3, 3), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #max pooling with a 2 by 2 filter\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#Output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#Learning rate for optimization, using stochastic gradient descent\n",
        "sgd = SGD(lr=0.0001)\n",
        "\n",
        "#Compiling model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ferS_zGJIAcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10\n",
        "\n",
        "Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
      ],
      "metadata": {
        "id": "ASsU05ZZIBDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First  convolutional layer with 32 filters\n",
        "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #max pooling with a 2 by 2 filter\n",
        "\n",
        "#Second  convolutional layer with 32 filters\n",
        "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #max pooling with a 2 by 2 filter\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu')) # First fully-connected layer of 128 neurons.\n",
        "model.add(Dense(128, activation='relu')) # Second fully-connected layer of 128 neurons.\n",
        "\n",
        "#Output layer\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "#Learning rate for optimization, using stochastic gradient descent\n",
        "sgd = SGD(lr=0.0001)\n",
        "\n",
        "#Compiling model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "bCw5hN8iIFvJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}